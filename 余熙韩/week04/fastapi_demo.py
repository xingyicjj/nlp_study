import torch

from fastapi import FastAPI
from transformers import BertTokenizer, BertForSequenceClassification
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Union, Optional

import sys
sys.path.append('../')

from utils.device_utils import get_device_info




class TextClassifyRequest(BaseModel):
    """
    è¯·æ±‚æ ¼å¼
    """
    request_id: Optional[str] = Field(..., description="è¯·æ±‚id, æ–¹ä¾¿è°ƒè¯•")
    request_text: Union[str, List[str]] = Field(..., description="è¯·æ±‚æ–‡æœ¬ã€å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")


class TextClassifyResponse(BaseModel):
    """
    æ¥å£è¿”å›æ ¼å¼
    """
    request_id: Optional[str] = Field(..., description="è¯·æ±‚id")
    request_text: Union[str, List[str]] = Field(..., description="è¯·æ±‚æ–‡æœ¬ã€å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")
    classify_result: Union[str, List[str]] = Field(..., description="åˆ†ç±»ç»“æœ")
    classify_time: float = Field(..., description="åˆ†ç±»è€—æ—¶")
    error_msg: str = Field(..., description="å¼‚å¸¸ä¿¡æ¯")



class SentimentAnalyzer:
    """å¤–å–è¯„è®ºæƒ…æ„Ÿåˆ†æå™¨"""

    def __init__(self, model_path, tokenizer_path):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            tokenizer_path: åˆ†è¯å™¨è·¯å¾„
        """
        # è·å–è®¾å¤‡
        self.device, self.device_type, _ = get_device_info()
        print(f"ğŸ¯ åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {self.device_type}")

        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)
        self.model = BertForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    def predict_single(self, text):
        """
        é¢„æµ‹å•æ¡æ–‡æœ¬çš„æƒ…æ„Ÿ
        
        Args:
            text: è¾“å…¥çš„è¯„è®ºæ–‡æœ¬
            
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœå’Œç½®ä¿¡åº¦
        """
        # æ–‡æœ¬é¢„å¤„ç†
        inputs = self.tokenizer(text,
                                truncation=True,
                                padding=True,
                                max_length=128,
                                return_tensors="pt").to(self.device)

        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=-1)

            # è·å–é¢„æµ‹ç»“æœ
            # confidence = predictions.max().item()
            type = predictions.data.max(1, keepdim=True)[1].item()
        return {
            'text': text,
            'type': f"{type}",
        }

    def predict_batch(self, texts):
        """
        æ‰¹é‡é¢„æµ‹å¤šæ¡æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            list: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for text in texts:
            results.append(self.predict_single(text))
        return results


model_config = dict(best_model_path="./results/checkpoint-2400",tokenizer_path="../models/google-bert/bert-base-chinese")

model = SentimentAnalyzer(model_config['best_model_path'], model_config['tokenizer_path'])


# åˆ›å»º FastAPI åº”ç”¨å®ä¾‹
app = FastAPI()

# å®šä¹‰æ ¹è·¯å¾„çš„ GET è¯·æ±‚å¤„ç†å‡½æ•°
@app.get("/")
async def read_root():
    return {"Hello": "World"}

# å®šä¹‰ä¸€ä¸ª POST è¯·æ±‚å¤„ç†å‡½æ•°ï¼Œç”¨äºæ¥æ”¶æ–‡æœ¬å¹¶è¿”å›é¢„æµ‹ç»“æœ
@app.post("/predict")
async def predict(request: TextClassifyRequest):
    response = TextClassifyResponse(
        request_id=request.request_id,
        request_text=request.request_text,
        classify_result=model.predict_single(request.request_text).get("type"),
        classify_time=0,
        error_msg="",
    )
    return response


# å®šä¹‰ä¸€ä¸ª POST è¯·æ±‚å¤„ç†å‡½æ•°ï¼Œç”¨äºæ¥æ”¶æ–‡æœ¬åˆ—è¡¨å¹¶è¿”å›æ‰¹é‡é¢„æµ‹ç»“æœ
@app.post("/predict_batch")
async def predict_batch(request: TextClassifyRequest):
    response = TextClassifyResponse(
        request_id=request.request_id,
        request_text=request.request_text,
        classify_result=[item.get("type") for item in model.predict_batch(request.request_text)],
        classify_time=0,
        error_msg="",
    )
    return response



if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
